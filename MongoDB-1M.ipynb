{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.0 --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" pyspark-shell'\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "import sys, time \n",
    "sys.path.append(os.path.abspath(\"E:\\mongo-hadoop-master\\mongo-hadoop-master\\spark\\src\\main\\python\"))\n",
    "import pymongo_spark\n",
    "pymongo_spark.activate()\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms = SparkSession.builder.appName('Test').config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/TestDB.TestCollection\").config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/TestDB.TCollection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RC = [20,40,60,80,100]\n",
    "OC = [2000,4000,6000,8000,10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update Heavy (A)\n",
    "for r in RC:\n",
    "    df = ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).load()\n",
    "    cc=[]\n",
    "    qq=np.array(df.toPandas().values)\n",
    "    for n in np.unique(qq[:,4]):\n",
    "        m = str(n)\n",
    "        cc.append(m[m.find(\"'\")+1:m.find(\"'\",m.find(\"'\")+1)])\n",
    "    for o in OC:\n",
    "        df.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp\").mode('overwrite').save()\n",
    "        np.random.seed(1)\n",
    "        st = time.time()\n",
    "        for i in range(int(o/2)):\n",
    "            pipeline = \"{'$match': {'_id': ObjectId('%s')}}\"%cc[np.random.randint(r*10000)]\n",
    "            vv=ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).option(\"pipeline\", pipeline).load()\n",
    "            gj = vv.rdd\n",
    "            gj.cache()\n",
    "            vv=vv.withColumn('_3',lit('0'))\n",
    "            vv.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp\").mode('append').save()\n",
    "        et = time.time()-st\n",
    "        results.append(['A',r,o,et])\n",
    "print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read Heavy (B)\n",
    "for r in RC:\n",
    "    df = ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).load()\n",
    "    cc=[]\n",
    "    qq=np.array(df.toPandas().values)\n",
    "    for n in np.unique(qq[:,4]):\n",
    "        m = str(n)\n",
    "        cc.append(m[m.find(\"'\")+1:m.find(\"'\",m.find(\"'\")+1)])\n",
    "    for o in OC:\n",
    "        wp=o/20\n",
    "        df.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp\").mode('overwrite').save()\n",
    "        np.random.seed(1)\n",
    "        st = time.time()\n",
    "        l = 0\n",
    "        for i in range(int(o*0.95)):\n",
    "            pipeline = \"{'$match': {'_id': ObjectId('%s')}}\"%cc[np.random.randint(r*10000)]\n",
    "            vv=ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).option(\"pipeline\", pipeline).load()\n",
    "            gh = vv.rdd\n",
    "            gh.cache()\n",
    "            if l%wp ==0:\n",
    "                vv=vv.withColumn('_3',lit('0'))\n",
    "                vv.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp\").mode('append').save()\n",
    "            l+=1\n",
    "        et = time.time()-st\n",
    "        results.append(['B',r,o,et])\n",
    "print(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read Only (C)\n",
    "for r in RC:\n",
    "    df = ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).load()\n",
    "    cc=[]\n",
    "    qq=np.array(df.toPandas().values)\n",
    "    for n in np.unique(qq[:,4]):\n",
    "        m = str(n)\n",
    "        cc.append(m[m.find(\"'\")+1:m.find(\"'\",m.find(\"'\")+1)])\n",
    "    for o in OC:\n",
    "        np.random.seed(1)\n",
    "        st = time.time()\n",
    "        for i in range(o):\n",
    "            pipeline = \"{'$match': {'_id': ObjectId('%s')}}\"%cc[np.random.randint(r*10000)]\n",
    "            vv=ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).option(\"pipeline\", pipeline).load()\n",
    "            gh = vv.rdd\n",
    "            gh.cache()\n",
    "        et = time.time()-st\n",
    "        results.append(['C',r,o,et])\n",
    "print(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read Latest (D)\n",
    "for r in RC:\n",
    "    df = ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).load()\n",
    "    cc=[]\n",
    "    qq=np.array(df.toPandas().values)\n",
    "    for n in np.unique(qq[:,4]):\n",
    "        m = str(n)\n",
    "        cc.append(m[m.find(\"'\")+1:m.find(\"'\",m.find(\"'\")+1)])\n",
    "    for o in OC:\n",
    "        wp=o/20\n",
    "        np.random.seed(1)\n",
    "        st = time.time()\n",
    "        l = 0\n",
    "        for i in range(int(o*0.95)):\n",
    "            pipeline = \"{'$match': {'_id': ObjectId('%s')}}\"%cc[l]\n",
    "            vv=ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).option(\"pipeline\", pipeline).load()\n",
    "            gh = vv.rdd\n",
    "            gh.cache()\n",
    "            if l%wp ==0:\n",
    "                vv.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp2\").mode('overwrite').save()\n",
    "            l+=1\n",
    "        et = time.time()-st\n",
    "        results.append(['D',r,o,et])\n",
    "print(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Short Ranges (E)\n",
    "for r in RC:\n",
    "    df = ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).load()\n",
    "    cc=[]\n",
    "    qq=np.array(df.toPandas().values)\n",
    "    for n in np.unique(qq[:,4]):\n",
    "        m = str(n)\n",
    "        cc.append(m[m.find(\"'\")+1:m.find(\"'\",m.find(\"'\")+1)])\n",
    "    for o in OC:\n",
    "        print(o,r)\n",
    "        wp=o/20\n",
    "        np.random.seed(1)\n",
    "        k=0\n",
    "        st = time.time()\n",
    "        for i in range(int(o*0.95)):\n",
    "            l = np.random.randint(r*10000-10)\n",
    "            for u in range(l,l+10):\n",
    "                pipeline = \"{'$match': {'_id': ObjectId('%s')}}\"%cc[u]\n",
    "                vv=ms.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.M%s\"%r).option(\"pipeline\", pipeline).load()\n",
    "                gh = vv.rdd\n",
    "                gh.cache()\n",
    "            gh.cache()\n",
    "            if k%wp ==0:\n",
    "                vv.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/TestDB.Temp2\").mode('overwrite').save()\n",
    "            l+=1\n",
    "            k+=1\n",
    "        et = time.time()-st\n",
    "        results.append(['E',r,o,et])\n",
    "print(\"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('DResults1m.csv',np.array(results),delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
